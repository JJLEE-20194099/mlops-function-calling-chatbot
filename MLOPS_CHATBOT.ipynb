{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38538308-567a-47e7-8cd9-93e03c2e4a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_252823/1325584172.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/tmp/ipykernel_252823/\u001b[0m\u001b[1;33m1325584172.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m5\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import threading\n",
    "import func_timeout\n",
    "import time\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from utils.meeyland_util import transferMeeyland\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from consume.utils import Redis\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0a288bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kafka:\n",
    "    def __init__(self, broker_id):\n",
    "        self.kafka_host = os.getenv('KAFKA_HOST')\n",
    "        self.broker_id = broker_id\n",
    "        self.kafka_port = os.getenv(f'KAFKA_PORT_{self.broker_id}')\n",
    "        self.producer = KafkaProducer(bootstrap_servers=['localhost:9092', 'localhost:9093', 'localhost:9094'])\n",
    "        #self.consumer = KafkaConsumer(bootstrap_servers=[f'{self.kafka_host}:{self.kafka_port}'], auto_offset_reset='earliest', enable_auto_commit=True, group_id=self.kafka_group_id,value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "\n",
    "    def kafka_consumer(self, kafka_group_id, kafka_topic):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            kafka_group_id (_type_): group id of consumer\n",
    "            kafka_topic (_type_): list topic to subscribe\n",
    "\n",
    "        Returns:\n",
    "            _type_: consumer\n",
    "        \"\"\"\n",
    "        consumer = KafkaConsumer(\n",
    "            bootstrap_servers=['localhost:9092', 'localhost:9093', 'localhost:9094'],\n",
    "            auto_offset_reset=\"earliest\",\n",
    "            enable_auto_commit=False,\n",
    "            group_id=kafka_group_id,\n",
    "            value_deserializer=lambda x: json.loads(x.decode(\"utf-8\")),\n",
    "            max_poll_records=10\n",
    "        )\n",
    "        consumer.subscribe(kafka_topic)\n",
    "        return consumer\n",
    "\n",
    "    def send_data(self, data,kafka_topic):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            data (_type_): data to send to kafka\n",
    "            kafka_topic (_type_): topic to send data\n",
    "\n",
    "        Returns:\n",
    "            _type_: False if send fail, True if send success\n",
    "        \"\"\"\n",
    "        status = self.producer.send(kafka_topic, value = json.dumps(data).encode('utf-8'))\n",
    "        self.producer.flush()\n",
    "        if status.is_done == True:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def create_consumer_and_subscribe(self, kafka_group_id, kafka_topic):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            kafka_group_id (_type_): group id of consumer\n",
    "            kafka_topic (_type_): list topic to subscribe\n",
    "\n",
    "        Returns:\n",
    "            _type_ : consumer\n",
    "        \"\"\"\n",
    "        consumer = KafkaConsumer(bootstrap_servers=['localhost:9092', 'localhost:9093', 'localhost:9094'], auto_offset_reset='earliest', enable_auto_commit=True, group_id=kafka_group_id,value_deserializer=lambda x: x.decode('utf-8'))\n",
    "        consumer.subscribe(kafka_topic)\n",
    "        return consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cbb639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyAiHLi5BQN2Truo7mrSpDRRo6G2TnnUGsA\")\n",
    "\n",
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51b0482-c574-4a86-9c1a-47937b3bd70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_gemini(input_sentence):\n",
    "    response = model.generate_content(\n",
    "        input_sentence,\n",
    "        safety_settings={\n",
    "            'HARM_CATEGORY_SEXUALLY_EXPLICIT':'block_none',\n",
    "            'HARM_CATEGORY_HATE_SPEECH':'block_none',\n",
    "            'HARM_CATEGORY_HARASSMENT':'block_none',\n",
    "            'HARM_CATEGORY_DANGEROUS_CONTENT':'block_none'\n",
    "        }\n",
    "    )\n",
    "    try:\n",
    "    # print(response.text)\n",
    "        return response.text\n",
    "    except:\n",
    "        print(response.prompt_feedback)\n",
    "        # return None\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99bdabc8-f309-42af-a495-dd436d3930bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_content_gemini(\"Crawl data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b751c7-9fb9-4c4e-8b10-1d4a5fa3ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_insights = \"\"\"\n",
    "Q: How to build MLOPs for predict realestate price in production?\n",
    "A: First. You have to crawl data. You extract, transform and insert to database. Moreover, you have to build training dataset to build AI model. To more efficiently, you can ensemble model to make predict result more stable\"\n",
    "-----------\n",
    "\n",
    "Q: What should I do after collecting data?\n",
    "A: Since the collected data has a lot of noise, the collected data needs to be cleaned first and put into a certain format. After the data cleaning step, the cleaned data can be stored in the database and used in the next stages.\n",
    "\n",
    "Q: With the data collected and newly updated into the database, it is possible to build a training set to train the model and continue to update the knowledge for the correct AI service?\n",
    "A: Of course. You can do anything on this clean data file, including training AI model. The process of processing data and building datasets for AI services, people go there is the process of building offline batch data: engineer feature / extract feature, transform feature, ...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87ab61d6-5b21-41d3-bc73-db52c0146856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crawl_data(source = 'meeyland'):\n",
    "    return f\"Start to crawl data from {source}\"\n",
    "\n",
    "def _clean_data(source = 'meeyland'):\n",
    "    return f\"Start to clean data from {source}\"\n",
    "\n",
    "def _insert_data(source = 'meeyland'):\n",
    "    return f\"Start to insert clean data to database\"\n",
    "\n",
    "def _build_offline_batch_data():\n",
    "    return f\"Build Offline batch data to train model\"\n",
    "\n",
    "def _train_price_prediction_model(model_name):\n",
    "    return f\"Start to train {model_name}\"\n",
    "\n",
    "def _get_information_about_train_experiment(experiment_id):\n",
    "    return f\"Get all metrics from {experiment_id}\"\n",
    "\n",
    "def _train_ensemble_model():\n",
    "    return f\"Start to train ensemble model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9afa38c0-8f93-4de1-aa5c-748bed8d0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_description = \"\"\"\n",
    "Function: _crawl_data\n",
    "    Description:\n",
    "        Crawl realestate data from source\n",
    "    Params:\n",
    "        source\n",
    "        - Enum: ['meeyland', 'muaban']\n",
    "        - Default: 'meeyland'\n",
    "        - Sample: 'meeyland'\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "\n",
    "Function: _clean_data\n",
    "    Description:\n",
    "        - Clean raw realestate data\n",
    "    Params:\n",
    "        source\n",
    "        - Enum: ['meeyland', 'muaban']\n",
    "        - Default: 'meeyland'\n",
    "        - Sample: 'meeyland'\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "Function: _insert_data\n",
    "    Description:\n",
    "        - Insert clean data to database\n",
    "    Params:\n",
    "        source\n",
    "        - Enum: ['meeyland', 'muaban']\n",
    "        - Default: 'meeyland'\n",
    "        - Sample: 'meeyland'\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "Function: _build_offline_batch_data\n",
    "    Decription:\n",
    "        - Build batch data for training AI model: extract feature, transform feature for training AI model phrase\n",
    "    Params:\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "Function: _train_price_prediction_model\n",
    "    Description:\n",
    "        - Training Price Prediction Model. Support models: lightgbm, catboost, xgboost\n",
    "    Params:\n",
    "        source\n",
    "        - Enum: ['cat', 'lgbm', 'xgb']\n",
    "        - Default: 'meeyland'\n",
    "        - Sample: 'meeyland'\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "Function: _get_information_about_train_experiment\n",
    "    Description:\n",
    "        Get machine learning metrics about train experiment:\n",
    "            - explained_variance\n",
    "            - neg_mean_absolute_percentage_error\n",
    "            - neg_root_mean_squared_error\n",
    "            - max_error\n",
    "    Params:\n",
    "        experiment_id\n",
    "        - string\n",
    "        - Default: \"hcm_knr_realestate_DATN_V4\"\n",
    "        - Sample: \"hcm_knr_realestate_DATN_V4\"\n",
    "    Output:\n",
    "        - Information about each training metrics\n",
    "\n",
    "Function: _train_ensemble_model\n",
    "    Description:\n",
    "        - Train ensemble model from single pretrained models: lgbm, xgb, ...\n",
    "    Params:\n",
    "    Output:\n",
    "        - None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f78f2245-7116-4ec8-8453-a8ae9cb216ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLLER_PROMPT_TEMPLATE = \"\"\"You are a controller, you receive below query from user, utilize the insights and choose what is the action from given functions\n",
    "\n",
    "Query: $$QUERY$$\n",
    "\n",
    "Insights: $$INSIGHTS$$\n",
    "\n",
    "List function:\n",
    "$$FUNCTIONS_DECRIPTION$$\n",
    "\n",
    "The response should be exactly like format and don't say anything else:\n",
    "```json\n",
    "{\n",
    "    \"observation\": <what is the current situation, what should follow>,\n",
    "    \"guidelines\": <what is the most suitable action in this situation and why>,\n",
    "    \"actions\": [{\n",
    "        \"fn\": <function name 1>,\n",
    "        \"params\": <function param 1>\n",
    "    }, {\n",
    "        \"fn\": <function name 2>,\n",
    "        \"params\": <function param 2>\n",
    "    }]\n",
    "}\n",
    "```\n",
    "RESPONSE:\n",
    "```json\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee27a9c8-cdce-42d2-a573-a91ec2b9aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_insights = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f6a78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "984da219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_candidate(obj):\n",
    "    actions = obj['actions']\n",
    "    try:\n",
    "        if len(actions):\n",
    "            return actions[0]\n",
    "    except:\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca36ebb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dca83c62-a88c-4476-81bc-3b4932b64aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"observation\": \"The user has a clean database from previous phrase and now wants to extract feature and build data for training phrase.\",\n",
      "    \"guidelines\": \"To extract feature and build data for training phrase, the most suitable action is to build offline batch data.\",\n",
      "    \"actions\": [{\n",
      "        \"fn\": \"_build_offline_batch_data\",\n",
      "        \"params\": {}\n",
      "    }]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to train model to predict realestate price. I have a clean database in previous phrase. Now  i want to extract feature and build data for training phrase\"\n",
    "test_inputs = CONTROLLER_PROMPT_TEMPLATE.replace(\"$$QUERY$$\", query).replace(\"$$INSIGHTS$$\", test_insights).replace(\"$$FUNCTIONS_DECRIPTION$$\", functions_description)\n",
    "# eval(generate_content_gemini(test_inputs))\n",
    "result = generate_content_gemini(test_inputs)\n",
    "\n",
    "print(result)\n",
    "result = json.loads(result.replace(\"`\", \"\").replace(\"\\n\", \"\"))\n",
    "# func_obj = get_best_candidate(result)\n",
    "# print(func_obj)\n",
    "\n",
    "# if func_obj['fn'] == '_crawl_data':\n",
    "#     print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "803e957a-cc49-43d4-b384-f5b2bd77b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Train ensemble model\"\n",
    "# test_inputs = CONTROLLER_PROMPT_TEMPLATE.replace(\"$$QUERY$$\", query).replace(\"$$INSIGHTS$$\", test_insights).replace(\"$$FUNCTIONS_DECRIPTION$$\", functions_description)\n",
    "# # eval(generate_content_gemini(test_inputs))\n",
    "# print(generate_content_gemini(test_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8646092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_func_obj_by_response(response):\n",
    "    result = json.loads(response.replace(\"`\", \"\").replace(\"\\n\", \"\"))\n",
    "    func_obj = get_best_candidate(result)\n",
    "    return func_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e079f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# from get_raw_data import crawl_meeyland_by_page\n",
    "# from clean_raw_data import processMeeyland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29cd0966-bb80-40a4-82a4-4cf8bcebf62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-06-25T18:18:14.242+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m380} INFO\u001b[0m - <BrokerConnection node_id=bootstrap-1 host=localhost:9093 <connecting> [IPv4 ('127.0.0.1', 9093)]>: connecting to localhost:9093 [('127.0.0.1', 9093) IPv4]\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:18:14.271+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m1205} INFO\u001b[0m - Probing node bootstrap-1 broker version\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:18:14.273+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m410} INFO\u001b[0m - <BrokerConnection node_id=bootstrap-1 host=localhost:9093 <connecting> [IPv4 ('127.0.0.1', 9093)]>: Connection complete.\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:18:14.383+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m1267} INFO\u001b[0m - Broker version identified as 2.5.0\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:18:14.385+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m1268} INFO\u001b[0m - Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-06-25T18:18:14.488+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m919} INFO\u001b[0m - <BrokerConnection node_id=0 host=localhost:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. \u001b[0m\n",
      "Running on local URL:  http://127.0.0.1:7863\n",
      "[\u001b[34m2024-06-25T18:18:15.536+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:18:15.546+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: GET http://127.0.0.1:7863/startup-events \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:18:17.602+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: HEAD http://127.0.0.1:7863/ \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:18:18.476+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "Running on public URL: https://666202a90cfcc93288.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "[\u001b[34m2024-06-25T18:18:20.772+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: HEAD https://666202a90cfcc93288.gradio.live \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://666202a90cfcc93288.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-06-25T18:23:14.561+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m380} INFO\u001b[0m - <BrokerConnection node_id=0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:23:14.562+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m410} INFO\u001b[0m - <BrokerConnection node_id=0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connection complete.\u001b[0m\n",
      "[\u001b[34m2024-06-25T18:23:14.563+0000\u001b[0m] {\u001b[34mconn.py:\u001b[0m919} INFO\u001b[0m - <BrokerConnection node_id=bootstrap-1 host=localhost:9093 <connected> [IPv4 ('127.0.0.1', 9093)]>: Closing connection. \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "duplicate session: clean\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "KafkaInstance = Kafka(broker_id = 0)\n",
    "MAX_THREAD = 10\n",
    "\n",
    "def slow_echo(message, history):\n",
    "    query = message\n",
    "    promp_with_input = CONTROLLER_PROMPT_TEMPLATE.replace(\"$$QUERY$$\", query).replace(\"$$INSIGHTS$$\", test_insights).replace(\"$$FUNCTIONS_DECRIPTION$$\", functions_description)\n",
    "    response = generate_content_gemini(promp_with_input)\n",
    "    try:\n",
    "        func_obj = get_func_obj_by_response(response)\n",
    "    except:\n",
    "        for retry in range(3):\n",
    "            func_obj = get_func_obj_by_response(response)\n",
    "            break\n",
    "\n",
    "    if func_obj['fn'] == '_crawl_data':\n",
    "        # total_data_count = 0\n",
    "        # for page in tqdm(range(200, 203)):\n",
    "        #     data = crawl_meeyland_by_page(page)\n",
    "        #     total_data_count += len(data)\n",
    "        #     if len(data):\n",
    "        #         reply = f'Crawling batch {len(data)} realestates - Here is a Realestate Title Example: {data[0]}'\n",
    "        #         yield reply\n",
    "        #     else:\n",
    "        #         yield \"Crawling...\"\n",
    "        # yield f\"Crawled {total_data_count}\"\n",
    "\n",
    "        os.system(\"tmux new-session -d -s crawl 'python src/helpers/chat_get_data.py'\")\n",
    "        yield \"Crawl Job Starting...\"\n",
    "\n",
    "    elif func_obj['fn'] == \"_clean_data\":\n",
    "        # consumer = KafkaInstance.kafka_consumer(\"raw_meeyland\", [\"raw_meeyland\"])\n",
    "        # cnt = 0\n",
    "        # for msg in tqdm(consumer):\n",
    "\n",
    "        #     if Redis().check_id_exist(f'meeyland_offset_{msg.offset}', 'meeyland_clean_rawdata'):\n",
    "        #         print(\"Ignore Processed Messages\")\n",
    "        #         continue\n",
    "        #     Redis().add_id_to_set(f'meeyland_offset_{msg.offset}', 'meeyland_clean_rawdata')\n",
    "        #     clean_msg = processMeeyland(msg)\n",
    "        #     try:\n",
    "        #         yield f'{clean_msg[\"propertyBasicInfo\"]}'\n",
    "        #     except:pass\n",
    "        #     cnt += 1\n",
    "        #     if cnt >= 3:\n",
    "        #         break\n",
    "        os.system(\"tmux new-session -d -s clean 'python src/helpers/chat_clean_data.py'\")\n",
    "        yield \"Clean Job Starting...\"\n",
    "    elif func_obj['fn'] == \"_insert_data\":\n",
    "        os.system(\"tmux new-session -d -s insert 'python src/helpers/chat_insert_data.py'\")\n",
    "        yield \"Insert to Database Job Starting...\"\n",
    "\n",
    "    elif func_obj[\"fn\"] == \"_build_offline_batch_data\":\n",
    "        yield \"Extract Feature Job Starting...\"\n",
    "\n",
    "        bkprice_server = os.getenv(\"BKPRICE_SERVER\")\n",
    "        url = f\"{bkprice_server}/build-offline-batch-data\"\n",
    "\n",
    "\n",
    "        payload = {}\n",
    "        headers = {}\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "        response = response.json()\n",
    "        example = response['sample_data']\n",
    "\n",
    "        url = f\"{bkprice_server}/build-offline-batch-data\"\n",
    "\n",
    "        payload = {}\n",
    "        headers = {}\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "        response = response.json()\n",
    "\n",
    "        yield f\"Here is an example: {example}\"\n",
    "\n",
    "\n",
    "    # yield str(func_obj)\n",
    "gr.ChatInterface(slow_echo).launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
