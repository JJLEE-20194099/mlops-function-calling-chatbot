{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38538308-567a-47e7-8cd9-93e03c2e4a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_3143476/3108997990.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">5</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/tmp/ipykernel_3143476/\u001b[0m\u001b[1;33m3108997990.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m5\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import threading\n",
    "import func_timeout\n",
    "import time\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from consume.utils import Redis\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbb639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-06-27T17:14:28.029+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyAiHLi5BQN2Truo7mrSpDRRo6G2TnnUGsA\")\n",
    "\n",
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51b0482-c574-4a86-9c1a-47937b3bd70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_gemini(input_sentence):\n",
    "    response = model.generate_content(\n",
    "        input_sentence,\n",
    "        safety_settings={\n",
    "            'HARM_CATEGORY_SEXUALLY_EXPLICIT':'block_none',\n",
    "            'HARM_CATEGORY_HATE_SPEECH':'block_none',\n",
    "            'HARM_CATEGORY_HARASSMENT':'block_none',\n",
    "            'HARM_CATEGORY_DANGEROUS_CONTENT':'block_none'\n",
    "        }\n",
    "    )\n",
    "    try:\n",
    "    # print(response.text)\n",
    "        return response.text\n",
    "    except:\n",
    "        print(response.prompt_feedback)\n",
    "        # return None\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bdabc8-f309-42af-a495-dd436d3930bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_content_gemini(\"Crawl data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b751c7-9fb9-4c4e-8b10-1d4a5fa3ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_insights = \"\"\"\n",
    "Q: What can I do with you?\n",
    "A: I am a chatbot programmed by Mr. Le Thanh Long during his thesis at Hanoi University of Science and Technology.\n",
    "\n",
    "Q: Which realestate data source you supported to crawl\n",
    "A: In demo version, you can crawl data from meeyland source\n",
    "\n",
    "Q: How to build MLOPs for predict realestate price in production?\n",
    "A: First. You have to crawl data. You extract, transform and insert to database. Moreover, you have to build training dataset to build AI model. To more efficiently, you can ensemble model to make predict result more stable\"\n",
    "-----------\n",
    "\n",
    "Q: What should I do after collecting data?\n",
    "A: Since the collected data has a lot of noise, the collected data needs to be cleaned first and put into a certain format. After the data cleaning step, the cleaned data can be stored in the database and used in the next stages.\n",
    "\n",
    "-----------\n",
    "Q: With the data collected and newly updated into the database, it is possible to build a training set to train the model and continue to update the knowledge for the correct AI service?\n",
    "A: Of course. You can do anything on this clean data file, including training AI model. The process of processing data and building datasets for AI services, people go there is the process of building offline batch data: engineer feature / extract feature, transform feature, ...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ab61d6-5b21-41d3-bc73-db52c0146856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crawl_data(source = 'meeyland'):\n",
    "    return f\"Start to crawl data from {source}\"\n",
    "\n",
    "def _clean_data(source = 'meeyland'):\n",
    "    return f\"Start to clean data from {source}\"\n",
    "\n",
    "def _insert_data(source = 'meeyland'):\n",
    "    return f\"Start to insert clean data to database\"\n",
    "\n",
    "def _build_offline_batch_data():\n",
    "    return f\"Build Offline batch data to train model\"\n",
    "\n",
    "def _train_price_prediction_model(model_name):\n",
    "    return f\"Start to train {model_name}\"\n",
    "\n",
    "def _get_information_about_train_experiment(experiment_id):\n",
    "    return f\"Get all metrics from {experiment_id}\"\n",
    "\n",
    "def _train_ensemble_model():\n",
    "    return f\"Start to train ensemble model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9afa38c0-8f93-4de1-aa5c-748bed8d0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_description = \"\"\"\n",
    "Function: _crawl_data\n",
    "    Description:\n",
    "        Crawl realestate data from source\n",
    "    Params:\n",
    "        source\n",
    "        - Enum: ['meeyland', 'muaban']\n",
    "        - Default: 'meeyland'\n",
    "        - Sample: 'meeyland'\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "\n",
    "Function: _clean_data\n",
    "    Description:\n",
    "        - Clean raw realestate data\n",
    "    Params:\n",
    "        source\n",
    "        - Enum: ['meeyland', 'muaban']\n",
    "        - Default: 'meeyland'\n",
    "        - Sample: 'meeyland'\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "Function: _insert_data\n",
    "    Description:\n",
    "        - Insert clean data to database\n",
    "    Params:\n",
    "        source\n",
    "        - Enum: ['meeyland', 'muaban']\n",
    "        - Default: 'meeyland'\n",
    "        - Sample: 'meeyland'\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "Function: _build_offline_batch_data\n",
    "    Decription:\n",
    "        - Build batch data for training AI model: extract feature, transform feature for training AI model phrase\n",
    "    Params:\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "Function: _train_price_prediction_model\n",
    "    Description:\n",
    "        - Training Price Prediction Model. Support models: lightgbm, catboost, xgboost\n",
    "    Params:\n",
    "        source\n",
    "        - Enum: ['cat', 'lgbm', 'xgb']\n",
    "        - Default: 'meeyland'\n",
    "        - Sample: 'meeyland'\n",
    "    Output:\n",
    "        - None\n",
    "\n",
    "Function: _get_information_about_train_experiment\n",
    "    Description:\n",
    "        Get machine learning metrics about train experiment:\n",
    "            - explained_variance\n",
    "            - neg_mean_absolute_percentage_error\n",
    "            - neg_root_mean_squared_error\n",
    "            - max_error\n",
    "    Params:\n",
    "        experiment_id\n",
    "        - string\n",
    "        - Default: \"hcm_knr_realestate_DATN_V4\"\n",
    "        - Sample: \"hcm_knr_realestate_DATN_V4\"\n",
    "    Output:\n",
    "        - Information about each training metrics\n",
    "\n",
    "Function: _train_ensemble_model\n",
    "    Description:\n",
    "        - Train ensemble model from single pretrained models: lgbm, xgb, ...\n",
    "    Params:\n",
    "    Output:\n",
    "        - None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78f2245-7116-4ec8-8453-a8ae9cb216ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLLER_PROMPT_TEMPLATE = \"\"\"You are a controller, you receive below query from user, utilize the insights and choose what is the action from given functions\n",
    "\n",
    "Query: $$QUERY$$\n",
    "\n",
    "Insights: $$INSIGHTS$$\n",
    "\n",
    "List function:\n",
    "$$FUNCTIONS_DECRIPTION$$\n",
    "\n",
    "The response should be exactly like format and don't say anything else:\n",
    "```json\n",
    "{\n",
    "    \"observation\": <what is the current situation, what should follow>,\n",
    "    \"guidelines\": <what is the most suitable action in this situation and why>,\n",
    "    \"actions\": [{\n",
    "        \"fn\": <function name 1>,\n",
    "        \"params\": <function param 1>\n",
    "    }, {\n",
    "        \"fn\": <function name 2>,\n",
    "        \"params\": <function param 2>\n",
    "    }]\n",
    "}\n",
    "```\n",
    "RESPONSE:\n",
    "```json\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce8eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_PROMPT_TEMPLATE =\"\"\"\n",
    "You are a classifier, you receive the following query from the user,utilize the insights  and select which of the following actions the user request falls into is given in json format with the following {key:value} format: (key is the action type, value is the meaning of the action):\n",
    "{\n",
    "\"MLOPS\": request tasks related to data processing, data cleaning, storage, data building to perform AI model training related to real estate,\n",
    "\"OTHERS\": Other types of questions include: what can you do?\n",
    "}\n",
    "\n",
    "Query: $$QUERY$$\n",
    "\n",
    "Insights: $$INSIGHTS$$\n",
    "\n",
    "Please return a single string of the type of the query. Do not return anything else.\n",
    "\n",
    "RESPONSE: action_type\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5993ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAL_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a chatbot were built by Mr. Le Thanh Long for research purposes during his thesis at Hanoi University of Science and Technology. You can use the following insights to answer questions from users.\n",
    "If there is no information in the insight, please answer according to your knowledge. And remember that you answer questions about Machine Learning Operations and AI algorithms in the real estate field.\n",
    "\n",
    "\n",
    "Query: $$QUERY$$\n",
    "\n",
    "Insights: $$INSIGHTS$$\n",
    "\n",
    "RESPONSE:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee27a9c8-cdce-42d2-a573-a91ec2b9aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_insights = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6a78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984da219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_candidate(obj):\n",
    "    actions = obj['actions']\n",
    "    try:\n",
    "        if len(actions):\n",
    "            return actions[0]\n",
    "    except:\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca36ebb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca83c62-a88c-4476-81bc-3b4932b64aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OTHERS'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What can you do?\"\n",
    "test_inputs = CLASSIFIER_PROMPT_TEMPLATE.replace(\"$$QUERY$$\", query).replace(\"$$INSIGHTS$$\", test_insights).replace(\"$$FUNCTIONS_DECRIPTION$$\", functions_description)\n",
    "# eval(generate_content_gemini(test_inputs))\n",
    "response = generate_content_gemini(test_inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "803e957a-cc49-43d4-b384-f5b2bd77b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What can I do with you?\"\n",
    "# test_inputs = FORMAL_PROMPT_TEMPLATE.replace(\"$$QUERY$$\", query).replace(\"$$INSIGHTS$$\", test_insights)\n",
    "# result = generate_content_gemini(test_inputs)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8646092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_func_obj_by_response(response):\n",
    "    result = json.loads(response.replace(\"`\", \"\").replace(\"\\n\", \"\"))\n",
    "    func_obj = get_best_candidate(result)\n",
    "    return func_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91cf37b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a classifier, you receive the following query from the user,utilize the insights  and select which of the following actions the user request falls into is given in json format with the following {key:value} format: (key is the action type, value is the meaning of the action):\n",
      "{\n",
      "\"MLOPS\": request tasks related to data processing, data cleaning, storage, data building to perform AI model training related to real estate,\n",
      "\"OTHERS\": Other types of questions include: what can you do?\n",
      "}\n",
      "\n",
      "Query: $$QUERY$$\n",
      "\n",
      "Insights: $$INSIGHTS$$\n",
      "\n",
      "Please return a single string of the type of the query. Do not return anything else.\n",
      "\n",
      "RESPONSE: action_type\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(CLASSIFIER_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29cd0966-bb80-40a4-82a4-4cf8bcebf62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2024-06-27T17:14:33.976+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\u001b[0m\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "[\u001b[34m2024-06-27T17:14:34.081+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: GET http://127.0.0.1:7860/startup-events \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "[\u001b[34m2024-06-27T17:14:34.250+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "[\u001b[34m2024-06-27T17:14:36.584+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "[\u001b[34m2024-06-27T17:14:37.278+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "Running on public URL: https://17530a2a9504a05315.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "[\u001b[34m2024-06-27T17:14:39.667+0000\u001b[0m] {\u001b[34m_client.py:\u001b[0m1026} INFO\u001b[0m - HTTP Request: HEAD https://17530a2a9504a05315.gradio.live \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://17530a2a9504a05315.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/chat_interface.py\", line 584, in _stream_fn\n",
      "    first_response = await async_iteration(generator)\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 656, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 649, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 635, in run_sync_iterator_async\n",
      "    raise StopAsyncIteration() from None\n",
      "StopAsyncIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/queueing.py\", line 532, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/blocks.py\", line 1526, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 656, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 782, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "RuntimeError: async generator raised StopAsyncIteration\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/chat_interface.py\", line 584, in _stream_fn\n",
      "    first_response = await async_iteration(generator)\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 656, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 649, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 635, in run_sync_iterator_async\n",
      "    raise StopAsyncIteration() from None\n",
      "StopAsyncIteration\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/queueing.py\", line 532, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/blocks.py\", line 1526, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 656, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/home/long/anaconda3/envs/mlops-env/lib/python3.10/site-packages/gradio/utils.py\", line 782, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "RuntimeError: async generator raised StopAsyncIteration\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "MAX_THREAD = 10\n",
    "\n",
    "def slow_echo(message, history):\n",
    "    query = message\n",
    "\n",
    "    classifier = CLASSIFIER_PROMPT_TEMPLATE.replace(\"$$QUERY$$\", query).replace(\"$$INSIGHTS$$\", test_insights)\n",
    "    response = generate_content_gemini(classifier)\n",
    "\n",
    "    response = response.replace(\"`\", \"\").replace(\"\\n\", \"\")\n",
    "    action_type = response\n",
    "    if action_type == 'MLOPS':\n",
    "\n",
    "        promp_with_input = CONTROLLER_PROMPT_TEMPLATE.replace(\"$$QUERY$$\", query).replace(\"$$INSIGHTS$$\", test_insights).replace(\"$$FUNCTIONS_DECRIPTION$$\", functions_description)\n",
    "        response = generate_content_gemini(promp_with_input)\n",
    "        try:\n",
    "            func_obj = get_func_obj_by_response(response)\n",
    "        except:\n",
    "            for retry in range(3):\n",
    "                func_obj = get_func_obj_by_response(response)\n",
    "                break\n",
    "\n",
    "        if func_obj is None:\n",
    "            yield \"I don't understand.\"\n",
    "        else:\n",
    "\n",
    "            if func_obj['fn'] == '_crawl_data':\n",
    "\n",
    "                os.system(\"tmux new-session -d -s crawl 'python chat_get_data.py'\")\n",
    "                yield \"Crawl Job Starting...\"\n",
    "\n",
    "            elif func_obj['fn'] == \"_clean_data\":\n",
    "                os.system(\"tmux new-session -d -s clean 'python chat_clean_data.py'\")\n",
    "                yield \"Clean Job Starting...\"\n",
    "            elif func_obj['fn'] == \"_insert_data\":\n",
    "                os.system(\"tmux new-session -d -s insert 'python chat_insert_data.py'\")\n",
    "                yield \"Insert to Database Job Starting...\"\n",
    "\n",
    "            elif func_obj[\"fn\"] == \"_build_offline_batch_data\":\n",
    "                yield \"Extract Feature Job Starting...\"\n",
    "\n",
    "                bkprice_server = os.getenv(\"BKPRICE_SERVER\")\n",
    "                url = f\"{bkprice_server}/build-offline-batch-data\"\n",
    "\n",
    "\n",
    "                payload = {}\n",
    "                headers = {}\n",
    "\n",
    "                response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "                response = response.json()\n",
    "                example = response['sample_data']\n",
    "\n",
    "                url = f\"{bkprice_server}/build-offline-batch-data\"\n",
    "\n",
    "                payload = {}\n",
    "                headers = {}\n",
    "                response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "                response = response.json()\n",
    "\n",
    "                yield f\"Here is an example: {example}\"\n",
    "            else:\n",
    "                query = message\n",
    "\n",
    "                classifier = FORMAL_PROMPT_TEMPLATE.replace(\"$$QUERY$$\", query).replace(\"$$INSIGHTS$$\", test_insights)\n",
    "                response = generate_content_gemini(classifier)\n",
    "\n",
    "                print(query, action_type)\n",
    "\n",
    "                yield response\n",
    "\n",
    "\n",
    "\n",
    "    # yield str(func_obj)\n",
    "gr.ChatInterface(slow_echo).launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
